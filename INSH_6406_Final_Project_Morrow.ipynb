{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Legislation Co-Sponsorship Network\n",
    "## I. Introduction\n",
    "Central to many theories of the policy-making process like the multiple-streams framework (MSF), advocacy coalition framework (ACF), and policy innovation, is an actor or set of actors that push forward policies. In MSF the actor or actors are known as policy entrepreneurs because they embed themselves in putting forward specfiic policy that they are invested in financially or ideologically. In ACF central actors are known as policy brokers that act as key individuals in the negotiation process between the competing coalitions. In policy innovation the actor invents new and innovative solutions to policy problems.\n",
    "\n",
    "While there are many studies looking at policy actors, only a handful use social network analysis as a methdology. Furthermore, the majority of studies are interested in federal policy, not state policy, or are or are focused on the broad frameworks rather than the specifics of the actor in relation to the network. Fewer still use co-sponsorship in their study as the edge definition. In this project, I use legislative data to look at California state co-sponsorship of bills for the congressional periods: 2009-2010, 2011-2012, 2013-2014, 2015-2016, and 2017-2018.\n",
    "\n",
    "My long-term overall research question is: can state legislation and legislative topics be explained and clarified by looking at the network structure of state legislative bodies? I am interested in resilience policy, so I am also interested in the applying the MSF and ACF frameworks to see if policy entrepreneurs are needed to pass certain resilience and disaster-policies, or if a focusing event is enough to pass policy. I think that looking at legislation networks can reveal underlying policy processes not obviously evident by looking at bill passages alone.\n",
    "\n",
    "I was not able to include all of my research goals in the time limitations for this project, so my research questions for this limited project are: \n",
    "1. What does the California legislation look like from a network perspective? \n",
    "2. Who are the most central legislators in the California state legislation? \n",
    "3. What is the average clustering coefficient of the five legislative periods I have data for and how has the clustering coefficient changed? I think the explanation for clustering coefficient changes is found in electoral changes (and topic foci, but that will require my full analysis goals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data\n",
    "The data I use is from Legiscan.com, a nonpartisan legislative tracking and reporting service that tracks legislation from all 50 states as well as the federal government and the District of Columbia. Legiscan has a database and an API for aquiring data, and I have downloaded the CSVs for the legislative sessions in question. The data includes three components: a spreadsheet of bills and their progress, a spreadsheet of legislators, and a spreadsheet of bill sponsors.\n",
    "\n",
    "The first step is to read files as pandas dataframes and lists. The functions below read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read files as pandas\n",
    "import pandas\n",
    "def filereader(file):\n",
    "    file = pandas.read_csv(file, encoding = 'utf-8', sep = ',')\n",
    "    return file\n",
    "\n",
    "# Function to read files as lists\n",
    "def listreader(file):\n",
    "    with open (file, 'r', encoding = 'utf-8') as f:\n",
    "        header = f.readline().strip().split(',')\n",
    "        data = [line.strip().split(',') for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "bills1718 = filereader('data/CA1718/bills.csv')\n",
    "people1718 = filereader('data/CA1718/people.csv')\n",
    "sponsors1718 = filereader('data/CA1718/sponsors.csv')\n",
    "sponsorslist1718 = listreader('data/CA1718/sponsors.csv')\n",
    "\n",
    "bills1516 = filereader('data/CA1516/bills.csv')\n",
    "people1516 = filereader('data/CA1516/people.csv')\n",
    "sponsors1516 = filereader('data/CA1516/sponsors.csv')\n",
    "sponsorslist1516 = listreader('data/CA1516/sponsors.csv')\n",
    "\n",
    "bills1314 = filereader('data/CA1314/bills.csv')\n",
    "people1314 = filereader('data/CA1314/people.csv')\n",
    "sponsors1314 = filereader('data/CA1314/sponsors.csv')\n",
    "sponsorslist1314 = listreader('data/CA1314/sponsors.csv')\n",
    "\n",
    "bills1112 = filereader('data/CA1112/bills.csv')\n",
    "people1112 = filereader('data/CA1112/people.csv')\n",
    "sponsors1112 = filereader('data/CA1112/sponsors.csv')\n",
    "sponsorslist1112 = listreader('data/CA1112/sponsors.csv')\n",
    "\n",
    "bills0910 = filereader('data/CA0910/bills.csv')\n",
    "people0910 = filereader('data/CA0910/people.csv')\n",
    "sponsors0910 = filereader('data/CA0910/sponsors.csv')\n",
    "sponsorslist0910 = listreader('data/CA0910/sponsors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bills dataframe looks like this (though I did not use it for the scope of this project, but plan on using it later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills1718[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legislators dataframe, note that each name is also coupled with a 'people_id':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people1718[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the sponsors dataframe is below. Note that for bills with multiple positions, there are multiple positions, meaning those bills are being co-sponsored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors1718[:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Steps Toward Analysis\n",
    "\n",
    "With the bill, legislator, and sponsor data loaded into memory, the next steps are:\n",
    "1. Match names with the sponsors list.\n",
    "2. Create an edgelist that counts the number of co-sponsorships. The number of bills co-sponsored by a dyad of legislators will be the edge weight.\n",
    "3. Create network visulizations and calculate network statistics (centrality and clustering).\n",
    "\n",
    "First, the analysis needs functions that create a dictionary of only the 'name' and the 'people_id', converts the dictionary to a list, and then converts the dictionary to a dataframe. We will do run these functions for every congressional session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dictionary of two items\n",
    "def dictcreator(input1, input2):\n",
    "    newdict = dict(zip(input1, input2))\n",
    "    return newdict\n",
    "peopledict1718 = dictcreator(people1718['name'], people1718['people_id'])\n",
    "peopledict1516 = dictcreator(people1516['name'], people1516['people_id'])\n",
    "peopledict1314 = dictcreator(people1314['name'], people1314['people_id'])\n",
    "peopledict1112 = dictcreator(people1112['name'], people1112['people_id'])\n",
    "peopledict0910 = dictcreator(people0910['name'], people0910['people_id'])\n",
    "\n",
    "# Function to convert dictionaries to lists\n",
    "def dicttolist(inputdict):\n",
    "    newlist = []\n",
    "    for key, value in inputdict.items():\n",
    "        newlist.append([key,value])\n",
    "    return newlist\n",
    "peoplelist1718 = dicttolist(peopledict1718)\n",
    "peoplelist1516 = dicttolist(peopledict1516)\n",
    "peoplelist1314 = dicttolist(peopledict1314)\n",
    "peoplelist1112 = dicttolist(peopledict1112)\n",
    "peoplelist0910 = dicttolist(peopledict0910)\n",
    "\n",
    "# Function to convert dictionary to dataframe\n",
    "def dfconverter(inputlist, colname1, colname2):\n",
    "    newdf = pandas.DataFrame(inputlist, columns=(colname1, colname2))\n",
    "    return newdf\n",
    "peopledf1718 = dfconverter(peoplelist1718, 'name', 'pid')\n",
    "peopledf1516 = dfconverter(peoplelist1516, 'name', 'pid')\n",
    "peopledf1314 = dfconverter(peoplelist1314, 'name', 'pid')\n",
    "peopledf1112 = dfconverter(peoplelist1112, 'name', 'pid')\n",
    "peopledf0910 = dfconverter(peoplelist0910, 'name', 'pid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the sponsors list needs to be matched with the list of legislators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match sponsors list with list of legislators - returns a list and a pandas dataframe.\n",
    "def matcher(list1, list2):\n",
    "    for items in list1:\n",
    "        for name, pid in list2:\n",
    "            if int(items[1]) == int(pid):\n",
    "                items.append(name)\n",
    "    return list1, pandas.DataFrame(list1, columns=('bill','pid','position','sponsor'))\n",
    "sponsoredlist1718, sponsoreddf1718 = matcher(sponsorslist1718, peoplelist1718)\n",
    "sponsoredlist1516, sponsoreddf1516 = matcher(sponsorslist1516, peoplelist1516)\n",
    "sponsoredlist1314, sponsoreddf1314 = matcher(sponsorslist1314, peoplelist1314)\n",
    "sponsoredlist1112, sponsoreddf1112 = matcher(sponsorslist1112, peoplelist1112)\n",
    "sponsoredlist0910, sponsoreddf0910 = matcher(sponsorslist0910, peoplelist0910)\n",
    "\n",
    "sponsoredlist1718[:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to create an edgelist out of the above list. The edgelist will match co-sponsors into dyads. The function below takes a long time, especially for all 5 legislative sessions. The function returns both a list and a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create an edgelist - Takes a very long time\n",
    "def edges(list1, list2):\n",
    "    edgelist = []\n",
    "    edgedict = {}\n",
    "    for bills in list1:\n",
    "        for names in list2:\n",
    "            try:\n",
    "                if bills[0] == names[0]:\n",
    "                    if bills[3] != 0:\n",
    "                        if bills[3] != names[3]:\n",
    "                            edgelist.append([bills[3], names[3]])\n",
    "                            edgedict[bills[3]] = names[3]\n",
    "            except:\n",
    "                continue \n",
    "    return edgelist, edgedict\n",
    "\n",
    "edgelist1718, edgedict1718 = edges(sponsoredlist1718, sponsoredlist1718)\n",
    "edgelist1516, edgedict1516 = edges(sponsoredlist1516, sponsoredlist1516)\n",
    "edgelist1314, edgedict1314 = edges(sponsoredlist1314, sponsoredlist1314)\n",
    "edgelist1112, edgedict1112 = edges(sponsoredlist1112, sponsoredlist1112)\n",
    "edgelist0910, edgedict0910 = edges(sponsoredlist0910, sponsoredlist0910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what our sponsorlist looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist1718[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function converts the list into a pandas dataframe with a 'source' and 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgedf1718 = pandas.DataFrame(edgelist1718, columns=('source','target'))\n",
    "edgedf1516 = pandas.DataFrame(edgelist1516, columns=('source','target'))\n",
    "edgedf1314 = pandas.DataFrame(edgelist1314, columns=('source','target'))\n",
    "edgedf1112 = pandas.DataFrame(edgelist1112, columns=('source','target'))\n",
    "edgedf0910 = pandas.DataFrame(edgelist0910, columns=('source','target'))\n",
    "edgedf1718[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current edgelist is unweighted, so when converted into a network, multiple co-sponsors will only count as one tie. Therefore, the number of co-sponsorships in a dyad will need to be counted and assigned as the edge weight to the dyad. I use the 'collections' library with the 'Counter' function.\n",
    "\n",
    "Below, first the edgelist is converted into tuples. Then the 'Counter' function is used to count repeated edges. Finally, the weighted list is converted back into a list of our network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to weight edges\n",
    "import collections\n",
    "edgetuple1718 = tuple(tuple(x) for x in edgelist1718)\n",
    "edgetuple1516 = tuple(tuple(x) for x in edgelist1516)\n",
    "edgetuple1314 = tuple(tuple(x) for x in edgelist1314)\n",
    "edgetuple1112 = tuple(tuple(x) for x in edgelist1112)\n",
    "edgetuple0910 = tuple(tuple(x) for x in edgelist0910)\n",
    "\n",
    "weightededges1718 = list(collections.Counter(edgetuple1718).items())\n",
    "weightededges1516 = list(collections.Counter(edgetuple1516).items())\n",
    "weightededges1314 = list(collections.Counter(edgetuple1314).items())\n",
    "weightededges1112 = list(collections.Counter(edgetuple1112).items())\n",
    "weightededges0910 = list(collections.Counter(edgetuple0910).items())\n",
    "\n",
    "\n",
    "params1718 = list((x, y, int(v)) for (x,y), v in weightededges1718)\n",
    "params1516 = list((x, y, int(v)) for (x,y), v in weightededges1516)\n",
    "params1314 = list((x, y, int(v)) for (x,y), v in weightededges1314)\n",
    "params1112 = list((x, y, int(v)) for (x,y), v in weightededges1112)\n",
    "params0910 = list((x, y, int(v)) for (x,y), v in weightededges0910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final parameters variable looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1112[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Network Visualization and Statistics\n",
    "\n",
    "Now that we have converted our co-sponsorship data into a weighted edgelist, we can produce network visualizations and calculate network statistics. The first step is to import the python network analysis library 'networkx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce network visualizations, the following steps must be followed:\n",
    "1. Create empty network graphs.\n",
    "2. Assign nodes and node names.\n",
    "3. Assign weighted edges.\n",
    "4. Remove isolated nodes (otherwise our networks will not visualize correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Create empty network graphs\n",
    "G1718 = nx.Graph()\n",
    "G1516 = nx.Graph()\n",
    "G1314 = nx.Graph()\n",
    "G1112 = nx.Graph()\n",
    "G0910 = nx.Graph()\n",
    "\n",
    "# Step 2. Assign nodes and node names\n",
    "node_names1718 = [n[0] for n in peoplelist1718]\n",
    "node_names1516 = [n[0] for n in peoplelist1516]\n",
    "node_names1314 = [n[0] for n in peoplelist1314]\n",
    "node_names1112 = [n[0] for n in peoplelist1112]\n",
    "node_names0910 = [n[0] for n in peoplelist0910]\n",
    "G1718.add_nodes_from(node_names1718)\n",
    "G1516.add_nodes_from(node_names1516)\n",
    "G1314.add_nodes_from(node_names1314)\n",
    "G1112.add_nodes_from(node_names1112)\n",
    "G0910.add_nodes_from(node_names0910)\n",
    "\n",
    "# Step 3. Assign weighted edges\n",
    "G1718.add_weighted_edges_from(params1718)\n",
    "G1516.add_weighted_edges_from(params1516)\n",
    "G1314.add_weighted_edges_from(params1314)\n",
    "G1112.add_weighted_edges_from(params1112)\n",
    "G0910.add_weighted_edges_from(params0910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the new network data variables can be summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates network information variables\n",
    "G1718info = nx.info(G1718)\n",
    "G1516info = nx.info(G1516)\n",
    "G1314info = nx.info(G1314)\n",
    "G1112info = nx.info(G1112)\n",
    "G0910info = nx.info(G0910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1718info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network data can also be visualized. Visualization requires the 'matplotlib' library function 'pyplot', and so it is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot is for the 2017-2018 legislative session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns weight values to new variables\n",
    "elarge1718 = [(u, v) for (u, v, d) in G1718.edges(data=True) if d['weight'] > 300]\n",
    "esmall1718 = [(u, v) for (u, v, d) in G1718.edges(data=True) if d['weight'] <= 299]\n",
    "\n",
    "# Kamada Kawai is the kind of network visualization layout we will use, but there are many others\n",
    "pos = nx.kamada_kawai_layout(G1718)\n",
    "\n",
    "# Next the network is drawn by using the weights.\n",
    "nx.draw_networkx_nodes(G1718, pos, node_size=4)\n",
    "nx.draw_networkx_edges(G1718, pos, edgelist=elarge1718,width=.5)\n",
    "nx.draw_networkx_edges(G1718, pos, edgelist=esmall1718,width=.1, alpha=.75, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G1718, pos, font_size=25, font_family='sans-serif')\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second is the 2015-2016 session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns weight values to new variables\n",
    "elarge1516 = [(u, v) for (u, v, d) in G1516.edges(data=True) if d['weight'] > 300]\n",
    "esmall1516 = [(u, v) for (u, v, d) in G1516.edges(data=True) if d['weight'] <= 299]\n",
    "\n",
    "# Kamada Kawai is the kind of network visualization layout we will use, but there are many others\n",
    "pos = nx.kamada_kawai_layout(G1516)\n",
    "\n",
    "# Next the network is drawn by using the weights.\n",
    "nx.draw_networkx_nodes(G1516, pos, node_size=4)\n",
    "nx.draw_networkx_edges(G1516, pos, edgelist=elarge1516,width=.5)\n",
    "nx.draw_networkx_edges(G1516, pos, edgelist=esmall1516,width=.1, alpha=.75, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G1516, pos, font_size=25, font_family='sans-serif')\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the 2013-2014 session, note for this session, the large/small weight sensitivity for edges has been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns weight values to new variables\n",
    "elarge1314 = [(u, v) for (u, v, d) in G1314.edges(data=True) if d['weight'] > 250]\n",
    "esmall1314 = [(u, v) for (u, v, d) in G1314.edges(data=True) if d['weight'] <= 249]\n",
    "\n",
    "# Kamada Kawai is the kind of network visualization layout we will use, but there are many others\n",
    "pos = nx.kamada_kawai_layout(G1314)\n",
    "\n",
    "# Next the network is drawn by using the weights.\n",
    "nx.draw_networkx_nodes(G1314, pos, node_size=4)\n",
    "nx.draw_networkx_edges(G1314, pos, edgelist=elarge1314,width=.5)\n",
    "nx.draw_networkx_edges(G1314, pos, edgelist=esmall1314,width=.1, alpha=.75, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G1314, pos, font_size=25, font_family='sans-serif')\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2011-2012 session is next, again, the large/small weight edge sensitivity is changed to be even lower than 2013-2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns weight values to new variables\n",
    "elarge1112 = [(u, v) for (u, v, d) in G1112.edges(data=True) if d['weight'] > 200]\n",
    "esmall1112 = [(u, v) for (u, v, d) in G1112.edges(data=True) if d['weight'] <= 199]\n",
    "\n",
    "# Kamada Kawai is the kind of network visualization layout we will use, but there are many others\n",
    "pos = nx.kamada_kawai_layout(G1112)\n",
    "\n",
    "# Next the network is drawn by using the weights.\n",
    "nx.draw_networkx_nodes(G1112, pos, node_size=4)\n",
    "nx.draw_networkx_edges(G1112, pos, edgelist=elarge1112,width=.5)\n",
    "nx.draw_networkx_edges(G1112, pos, edgelist=esmall1112,width=.1, alpha=.75, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G1112, pos, font_size=25, font_family='sans-serif')\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the 2009-2010 session, again the weights are lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns weight values to new variables\n",
    "elarge0910 = [(u, v) for (u, v, d) in G0910.edges(data=True) if d['weight'] > 200]\n",
    "esmall0910 = [(u, v) for (u, v, d) in G0910.edges(data=True) if d['weight'] <= 199]\n",
    "\n",
    "# Kamada Kawai is the kind of network visualization layout we will use, but there are many others\n",
    "pos = nx.kamada_kawai_layout(G0910)\n",
    "\n",
    "# Next the network is drawn by using the weights.\n",
    "nx.draw_networkx_nodes(G0910, pos, node_size=4)\n",
    "nx.draw_networkx_edges(G0910, pos, edgelist=elarge0910,width=.5)\n",
    "nx.draw_networkx_edges(G0910, pos, edgelist=esmall0910,width=.1, alpha=.75, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_labels(G0910, pos, font_size=25, font_family='sans-serif')\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For statistics, there are two commonly-used measures of networks that are built into 'networkx'. First, the eigenvector centrality of the nodes is an indicator of the most central legislator in terms of not only its own centrality, but the centrality of its neighbors. Second, the clustering coefficent of the network  is an indicator of the overall edge compositions. Clustering is an statistic of how much the network is working together as a singular unit. Measuring network transitivity is an alternative measure of overall network clustering, but does not take into account edge weights.\n",
    "\n",
    "The first lines below calculates eigenvector centrality and outputs a dictionary. Then the dictionary is converted into an ordered list and sorted by centrality from highest to lowest. The numbers are low because the network is weighted and there are a large number of possible co-sponsorships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Eigenvector centrality.\n",
    "G1718eigenvectorcentrality = nx.algorithms.eigenvector_centrality(G1718, weight='weight')\n",
    "G1516eigenvectorcentrality = nx.algorithms.eigenvector_centrality(G1516, weight='weight')\n",
    "G1314eigenvectorcentrality = nx.algorithms.eigenvector_centrality(G1314, weight='weight')\n",
    "G1112eigenvectorcentrality = nx.algorithms.eigenvector_centrality(G1112, weight='weight')\n",
    "G0910eigenvectorcentrality = nx.algorithms.eigenvector_centrality(G0910, weight='weight')\n",
    "\n",
    "# Function that sorts dictionaries\n",
    "def sorter(input):\n",
    "    newlist = []\n",
    "    for key, value in list(input.items()):\n",
    "        newlist.append((value,key))\n",
    "    newlist.sort(reverse=True)\n",
    "    return newlist\n",
    "\n",
    "# Lines that produce sorted Eigenvector centrality lists\n",
    "G1718ECSorted = sorter(G1718eigenvectorcentrality)\n",
    "G1516ECSorted = sorter(G1516eigenvectorcentrality)\n",
    "G1314ECSorted = sorter(G1314eigenvectorcentrality)\n",
    "G1112ECSorted = sorter(G1112eigenvectorcentrality)\n",
    "G0910ECSorted = sorter(G0910eigenvectorcentrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2017-2018:', G1718ECSorted[:11],'\\n\\n' + '2015-2016:', G1516ECSorted[:11],'\\n\\n' +'2013-2014:', G1314ECSorted[:11],'\\n\\n' + '2011-2012:', G1112ECSorted[:11],'\\n\\n' + '2009-2010:', G0910ECSorted[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next statistic of interest is the average network clustering coeffient for all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1718clustering = nx.algorithms.cluster.average_clustering(G1718, weight='weight')\n",
    "G1516clustering = nx.algorithms.cluster.average_clustering(G1516, weight='weight')\n",
    "G1314clustering = nx.algorithms.cluster.average_clustering(G1314, weight='weight')\n",
    "G1112clustering = nx.algorithms.cluster.average_clustering(G1112, weight='weight')\n",
    "G0910clustering = nx.algorithms.cluster.average_clustering(G0910, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2017-2018:', G1718clustering,'\\n' + '2015-2016:', G1516clustering, '\\n' + '2013-2014:', G1314clustering, '\\n' + '2011-2012:', G1112clustering, '\\n' + '2009-2010:', G0910clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the average clustering coefficients visualized as a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ['2009-2010', '2011-2012', '2013-2014', '2015-2016', '2017-2018']\n",
    "avecluster = [G0910clustering, G1112clustering, G1314clustering, G1516clustering, G1718clustering]\n",
    "plt.plot(session, avecluster, color='green')\n",
    "plt.xlabel('Legislative Session')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]\n",
    "plt.title('Average Clustering Coefficient 2009-2018')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusions and Future Work\n",
    "\n",
    "The produced visualizations and descriptive statistics provide informative findings. Surprisingly, we do not see distinct clustering among groups within the California legislation. Instead, in 2017-2018 and 2013-2014 we see a core and periphery structure instead, and the other legislative sessions are more dispersed without any clear core or clusters. The lack of multiple clusters indicates two ideas worth investigating deeper. First, there is not any distinct partisan clustering and while I did not associate partisan labels with nodes, it appears that legislators are sponsoring bills across the aisle. Second, the California legislation is bicameral with an assembly and a senate, so there is co-sponsorship occuring across houses of the legislature.\n",
    "\n",
    "From the statistics, we can see that the 2015-2016 legislative session has the highest average clustering coefficient, meaning that they are working together the most on average to co-sponsor bills together. The 2009-2010 legislative session on the other hand, has the lowest average clustering coefficient. From the eigenvector centrality, we can draw out the most central legislators for each legislative session, and some of the legislators appear more than once in the top ten central legislators.\n",
    "\n",
    "A lot of this project was spent converting the data into edgelists, but there is still a lot to be done in the future to fulfill the overall objectives of the project. Next, I would like to incorporate the bill data to associate sponsorships with the bills themselves. I can then see if sponsorship numbers, centrality, or clustering is correlates with the passage of a bill. Furthermore, I would like to perform a text analysis of the bills themselves to see if certain legislators are associated with different kinds of bills outside of typical committee structure. I can also use the network data separated by bill topic to see which topics are polarized. There is also the potential of comparing networks across states. The network data and statistics is only the first step in what can branch out into many different parts of a larger project looking at state legislation. For example, other states could be included to compare sponsorship networks across states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
